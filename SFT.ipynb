{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKgLHjUfloYH"
   },
   "source": [
    "## **Supervised Fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBYDF_jrixk6"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mS-54D2hjTTx",
    "outputId": "999fae5e-a2bc-4547-8e96-cd034fd9ecba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcquFjzGjb96",
    "outputId": "7b64bcb3-0d44-4629-b4d1-66ba34ebdf16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfV2Mxfcl4P0"
   },
   "source": [
    "testing the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOpmqDz4klNW",
    "outputId": "6403941d-b3fe-4ba1-b573-d85645f3f3e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [15496, 11, 10318, 361, 26494, 375, 428, 1735, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, Saif Rathod this side!\"\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)\n",
    "#15496, 11, 10318, 361, 26494, 375, 428, 1735, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "mO-k6RsnnlFM",
    "outputId": "ce270abc-fc75-48eb-f5eb-4f8167ff39f5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Hello, Saif Rathod this side!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6Q-PXvbou2Y",
    "outputId": "b9e160d3-179e-42eb-8add-ad45aebc08bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.7.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hawpHSBxu0bD"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADEvJzqhwASn",
    "outputId": "c0a9046b-1695-475c-be69-1b52ab8e93f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reHecWsfvfQE"
   },
   "outputs": [],
   "source": [
    "ds_train, ds_val = ds['train'], ds['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6PKYVb4vpb9",
    "outputId": "310b86c1-97a3-4d23-c937-d736f1082e4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 6,\n",
       " 'sentence': 'demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . ',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KZj6Az0mvvKh",
    "outputId": "630a4b45-1a66-41fc-c849-cdef02102252"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " 'sentence': ['hide new secretions from the parental units ',\n",
       "  'contains no wit , only labored gags ',\n",
       "  'that loves its characters and communicates something rather beautiful about human nature ',\n",
       "  'remains utterly satisfied to remain the same throughout ',\n",
       "  'on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ',\n",
       "  \"that 's far too tragic to merit such superficial treatment \",\n",
       "  'demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . ',\n",
       "  'of saucy ',\n",
       "  \"a depressed fifteen-year-old 's suicidal poetry \",\n",
       "  \"are more deeply thought through than in most ` right-thinking ' films \"],\n",
       " 'label': [0, 0, 1, 0, 0, 0, 1, 1, 0, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-t6_9ua2wOpv"
   },
   "source": [
    "tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "06f12cb3a1954ac7b19510b562cc62cc",
      "5db6a269142b43e5ad051824f35ac8cf",
      "133d7e18df3945e796321361ebf80ed9",
      "edb16c9f0fc543d5ae6965b1a123b124",
      "0fe5434ed2a446a7ad966c33b217d346",
      "657ff8f73ccc45d3963a24b575b98eb2",
      "991bffdb76db445195fe258aa2279e52",
      "193730622a1a4b81a065e5313104c4dc",
      "32edf7e4bed94fbab7cfd0cb9bb9023d",
      "68a4c3fc9ba84f959c335c843c4151f3",
      "9abc8a28030c4084afc931ed5fa50f76",
      "bdaa3a77992a422499004fa5f4f7b77f",
      "596733861de84dada49bf3437632bcec",
      "a3e8bc0f9dbe4200ba3249911fade181",
      "8e97dc430d1f45e2ae01a7cb949e5eb2",
      "78dfafd04d674c268b06037bcfe345c2",
      "f130537b1d7d406cb8c5b5b378309b94",
      "120ba7c7efa944789a68f170cbda380e",
      "71f73d2b77cd42328c45e0ed1e382f91",
      "e3267b9028234a5a91a491c412925513",
      "1108d8699b534b23a94916411633aa59",
      "a6a37b9fdf8745da9e303c7c9e0cda19"
     ]
    },
    "id": "2jVYnuVDv8U4",
    "outputId": "8c83456a-4412-4bcd-84bd-4eb541ab1710"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f12cb3a1954ac7b19510b562cc62cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdaa3a77992a422499004fa5f4f7b77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "  return tokenizer(batch['sentence'])\n",
    "\n",
    "map_kwargs = {\n",
    "    'batched' : True,\n",
    "    'batch_size' : 512,\n",
    "    'remove_columns' : ['idx', 'sentence', 'label']\n",
    "}\n",
    "\n",
    "tokenized_ds_train = ds_train.map(tokenize, **map_kwargs)\n",
    "tokenized_ds_val = ds_val.map(tokenize, **map_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSHVBnqfwfCB",
    "outputId": "3a1e7fe3-e0ae-451b-a131-a975391934e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [24717, 649, 3200, 507, 422, 262, 21694, 4991, 220],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "hMdLid21wefp",
    "outputId": "f80145fa-b8a4-42f9-8c4e-e781dffd23be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[5562,\n",
       "   705,\n",
       "   82,\n",
       "   1290,\n",
       "   1165,\n",
       "   15444,\n",
       "   284,\n",
       "   17004,\n",
       "   884,\n",
       "   31194,\n",
       "   3513,\n",
       "   220],\n",
       "  [26567,\n",
       "   2536,\n",
       "   689,\n",
       "   326,\n",
       "   262,\n",
       "   3437,\n",
       "   286,\n",
       "   884,\n",
       "   289,\n",
       "   31777,\n",
       "   2512,\n",
       "   30181,\n",
       "   355,\n",
       "   29408,\n",
       "   1830,\n",
       "   460,\n",
       "   991,\n",
       "   1210,\n",
       "   503,\n",
       "   257,\n",
       "   1402,\n",
       "   837,\n",
       "   2614,\n",
       "   2646,\n",
       "   351,\n",
       "   281,\n",
       "   7016,\n",
       "   3355,\n",
       "   404,\n",
       "   764,\n",
       "   220],\n",
       "  [1659, 473, 84, 948, 220],\n",
       "  [64, 19095, 17280, 12, 1941, 12, 727, 705, 82, 26781, 19518, 220],\n",
       "  [533,\n",
       "   517,\n",
       "   7744,\n",
       "   1807,\n",
       "   832,\n",
       "   621,\n",
       "   287,\n",
       "   749,\n",
       "   4600,\n",
       "   826,\n",
       "   12,\n",
       "   28973,\n",
       "   705,\n",
       "   7328,\n",
       "   220]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds_train[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oUjedkUxxkH3",
    "outputId": "e2c1a91c-3512-48ce-da2c-f8bc9d0d5714"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: that 's far too tragic to merit such superficial treatment \n",
      "Tokenized: demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \n",
      "Tokenized: of saucy \n",
      "Tokenized: a depressed fifteen-year-old 's suicidal poetry \n",
      "Tokenized: are more deeply thought through than in most ` right-thinking ' films \n"
     ]
    }
   ],
   "source": [
    "for i, seq in enumerate(tokenized_ds_train[5:10]['input_ids']):\n",
    "  print(f\"Tokenized: {tokenizer.decode(seq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NcVU-yzqyV2J",
    "outputId": "a0ceec89-1e82-4f50-923a-2dda57d667a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67349, 872)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_ds_train), len(tokenized_ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0c80279b24f04be5bda57169baca0e31",
      "abda53d8e9224408ad3150a6a9abb0d3",
      "d7272bf625a14e6c85eba5f5ced2b16f",
      "0d20d8b24d1848fba7886b01f7b47849",
      "aa0b973194f94017b9222b020a0c6cd5",
      "4574705cb0464256be79005151e700ce",
      "26f8cfdc5b394616bf78cd82afd5ed34",
      "b569589e8a584e69a7eb9b7af76216f2",
      "05cf29f04df94a3e9d76f133890fcb15",
      "3d763e8c66874c5eb767998068945f51",
      "565dbdacf9da451fa3e014a4b8c398a9"
     ]
    },
    "id": "DqrXNmPizMBk",
    "outputId": "5b39f281-5320-4ef0-97f2-f91c6e00674f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c80279b24f04be5bda57169baca0e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds_train = tokenized_ds_train.filter(lambda x: len(x['input_ids']) > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "f39b4575ce5e4e0e846acd99ef47a876",
      "c031595b27b341699462d78439f6ce98",
      "99fa6331ecac47e2ad54ada5ef5398b9",
      "503d0ef11ce4476e8085ad16ec922319",
      "271b190229ed4f7bb749eec3fb907a3e",
      "c9a00cc195fc4d2aa1413c5a7f869bcf",
      "c8100507abc14f2f9d3ac934bec590b0",
      "0f2d391ab1814d8ca87926d6ec765dd8",
      "3e2d4a45aa584196b6deba9b078ae682",
      "5942743e2acf4adcb202952af0d25e40",
      "6c16afa8ca064ac08b9e7af85ae01de8"
     ]
    },
    "id": "R9mDXiNLzrbz",
    "outputId": "62321345-26de-4c58-c2bd-40695ae8c70d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39b4575ce5e4e0e846acd99ef47a876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds_val = tokenized_ds_val.filter(lambda x: len(x['input_ids']) > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_w6qWgOWzu-8",
    "outputId": "93e352d5-4702-412c-e6be-c06a4814eb51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49401, 867)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_ds_train), len(tokenized_ds_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVh7B_ryz1zb"
   },
   "source": [
    "## Preparing a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bpZedmdY22am",
    "outputId": "afa8a1fd-777c-4d90-e634-f9df01e4ae9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "6dmgdUgN3KR-",
    "outputId": "d6cea6a1-2802-48fe-8694-4663d0c1786d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Successfully uninstalled numpy-1.26.4\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Installing collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numpy && pip install numpy==1.26.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxQkM9cyz4vh"
   },
   "outputs": [],
   "source": [
    "tokenized_ds_train.set_format(type='torch')\n",
    "tokenized_ds_val.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qBWIoqbH0OMZ",
    "outputId": "b6ffc00a-6001-4552-ecf3-2269b6b749e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([24717,   649,  3200,   507,   422,   262, 21694,  4991,   220]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOt7aktZ0Wol",
    "outputId": "20c00158-9b91-44fc-a029-d5749dfadd1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [tensor([24717,   649,  3200,   507,   422,   262, 21694,  4991,   220]),\n",
       "  tensor([ 3642,  1299,   645, 20868,   837,   691,  2248,  1850,   308,  3775,\n",
       "            220]),\n",
       "  tensor([ 5562, 10408,   663,  3435,   290, 48556,  1223,  2138,  4950,   546,\n",
       "           1692,  3450,   220]),\n",
       "  tensor([ 2787,  1299, 15950, 11378,   284,  3520,   262,   976,  3690,   220]),\n",
       "  tensor([  261,   262,  5290, 15827,    12,  1659,    12,  1169,    12,  1008,\n",
       "           9310, 35478, 20954,   262, 28303,   714, 47478,   469,   510,   220])],\n",
       " 'attention_mask': [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2ceFrtk5Y1P"
   },
   "source": [
    "### Padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bs62hvKG5U9B",
    "outputId": "cc85fdc6-d43c-407e-db6b-2df91d32f449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F86Wncfd5e1v",
    "outputId": "7fd5ee65-b764-4e3a-8e2e-fcc297f6bd70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Xp0xTS97lkX"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzWADWCl88KM"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size' : 32,\n",
    "    'collate_fn' : data_collator\n",
    "}\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_ds_train, **dataloader_params)\n",
    "val_dataloader = DataLoader(tokenized_ds_val, **dataloader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHohbIjM6hkV",
    "outputId": "e658c488-3b3d-4c43-b7c1-20b80551de93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1544"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cH-lvve26lSt",
    "outputId": "a735501c-f49f-4abb-8154-3e742fe6c9c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49408"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1544 * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5U0B3_WG6uE0",
    "outputId": "879f2c52-1ebb-4a74-8655-ac8e5beaf118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[24717,   649,  3200,  ..., 50256, 50256, 50256],\n",
      "        [ 3642,  1299,   645,  ..., 50256, 50256, 50256],\n",
      "        [ 5562, 10408,   663,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [  672, 35260,   284,  ..., 50256, 50256, 50256],\n",
      "        [ 1169,  2104,   966,  ..., 50256, 50256, 50256],\n",
      "        [  292,   484,  1282,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[24717,   649,  3200,  ...,  -100,  -100,  -100],\n",
      "        [ 3642,  1299,   645,  ...,  -100,  -100,  -100],\n",
      "        [ 5562, 10408,   663,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [  672, 35260,   284,  ...,  -100,  -100,  -100],\n",
      "        [ 1169,  2104,   966,  ...,  -100,  -100,  -100],\n",
      "        [  292,   484,  1282,  ...,  -100,  -100,  -100]])})\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J__kg-ui62S3",
    "outputId": "48b8b5a0-9d04-4926-e690-26128cea3eb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape  #batch_size, seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2VUeAPy9Q7t",
    "outputId": "549b30b9-128f-45e1-f6dd-07076a29bc0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([24717,   649,  3200,   507,   422,   262, 21694,  4991,   220, 50256,\n",
       "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "        50256, 50256, 50256, 50256, 50256])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFnbQasB9Eat",
    "outputId": "0473775a-4f9c-4a35-cb67-ee0c03e1a002"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([24717,   649,  3200,   507,   422,   262, 21694,  4991,   220,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WIAPZ7wz9lGk",
    "outputId": "9e481ebe-69cd-45c8-b0fc-61b776457641"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gr3xuogV9wIb"
   },
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVNe2bZc9pIy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySC9jo-o-T9x"
   },
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEWGXgQ1A22Z"
   },
   "outputs": [],
   "source": [
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        # iteration = epoch * len(val_dataloader) + i\n",
    "        batch = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss # Uses transformers.loss.loss_utils.ForCausalLMLoss for loss calculation\n",
    "            total_loss += loss.item()\n",
    "    print(f'val_loss at {epoch} epoch:', total_loss / len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "J8nkMMi793ql",
    "outputId": "5631fa55-8de1-4a97-e462-fb2eedea4c1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss at 0 epoch: 3.934447339602879\n",
      "Loss: 3.4918212890625\n",
      "Loss: 3.730374813079834\n",
      "Loss: 3.4608428478240967\n",
      "Loss: 3.70367169380188\n",
      "Loss: 3.8544561862945557\n",
      "Loss: 3.8919858932495117\n",
      "Loss: 3.6999013423919678\n",
      "Loss: 3.6266391277313232\n",
      "Loss: 3.6039493083953857\n",
      "Loss: 3.6997900009155273\n",
      "Loss: 3.7023937702178955\n",
      "Loss: 3.4633944034576416\n",
      "Loss: 3.813965320587158\n",
      "Loss: 3.902867317199707\n",
      "Loss: 3.448918104171753\n",
      "Loss: 3.4941587448120117\n",
      "Loss: 3.593003511428833\n",
      "Loss: 3.7093873023986816\n",
      "Loss: 3.662118673324585\n",
      "Loss: 3.6940207481384277\n",
      "Loss: 3.622570753097534\n",
      "Loss: 3.442495584487915\n",
      "Loss: 3.6286044120788574\n",
      "Loss: 3.6547813415527344\n",
      "Loss: 3.6707489490509033\n",
      "Loss: 3.48372220993042\n",
      "Loss: 3.656770944595337\n",
      "Loss: 3.5214474201202393\n",
      "Loss: 3.6530158519744873\n",
      "Loss: 3.6252589225769043\n",
      "Loss: 3.6937968730926514\n",
      "Loss: 3.6192851066589355\n",
      "Loss: 3.4953272342681885\n",
      "Loss: 3.698779821395874\n",
      "Loss: 3.7242467403411865\n",
      "Loss: 3.5429930686950684\n",
      "Loss: 3.618696451187134\n",
      "Loss: 3.5880227088928223\n",
      "Loss: 3.658062219619751\n",
      "Loss: 3.4612209796905518\n",
      "Loss: 3.923133373260498\n",
      "Loss: 3.6581573486328125\n",
      "Loss: 3.5313899517059326\n",
      "Loss: 3.7765891551971436\n",
      "Loss: 3.6644365787506104\n",
      "Loss: 3.607261896133423\n",
      "Loss: 3.689628839492798\n",
      "Loss: 3.805271863937378\n",
      "Loss: 3.8784728050231934\n",
      "Loss: 3.567026376724243\n",
      "Loss: 3.5342133045196533\n",
      "Loss: 3.5547773838043213\n",
      "Loss: 3.6639320850372314\n",
      "Loss: 3.780170202255249\n",
      "Loss: 3.648221492767334\n",
      "Loss: 3.79502534866333\n",
      "Loss: 3.6115493774414062\n",
      "Loss: 3.683662176132202\n",
      "Loss: 3.8377270698547363\n",
      "Loss: 3.6176822185516357\n",
      "Loss: 3.5709149837493896\n",
      "Loss: 3.7346792221069336\n",
      "Loss: 3.924738883972168\n",
      "Loss: 3.5611963272094727\n",
      "Loss: 3.7671735286712646\n",
      "Loss: 3.4871890544891357\n",
      "Loss: 3.6518681049346924\n",
      "Loss: 3.483557939529419\n",
      "Loss: 3.5424656867980957\n",
      "Loss: 3.861419677734375\n",
      "Loss: 3.6276509761810303\n",
      "Loss: 3.8895599842071533\n",
      "Loss: 3.861898183822632\n",
      "Loss: 3.6327152252197266\n",
      "Loss: 3.662430763244629\n",
      "Loss: 3.677292823791504\n",
      "Loss: 3.3801991939544678\n",
      "Loss: 3.76188063621521\n",
      "Loss: 3.7323179244995117\n",
      "Loss: 3.7639801502227783\n",
      "Loss: 3.3823137283325195\n",
      "Loss: 3.903932571411133\n",
      "Loss: 3.663534164428711\n",
      "Loss: 3.7747857570648193\n",
      "Loss: 3.5463485717773438\n",
      "Loss: 3.566218376159668\n",
      "Loss: 3.693134307861328\n",
      "Loss: 3.495992422103882\n",
      "Loss: 3.8287832736968994\n",
      "Loss: 3.8163795471191406\n",
      "Loss: 3.6270060539245605\n",
      "Loss: 3.5855281352996826\n",
      "Loss: 3.6755869388580322\n",
      "Loss: 3.5981411933898926\n",
      "Loss: 3.475532293319702\n",
      "Loss: 3.7626569271087646\n",
      "Loss: 3.6771080493927\n",
      "Loss: 3.729172468185425\n",
      "Loss: 3.549067497253418\n",
      "Loss: 3.206367015838623\n",
      "Loss: 3.769411325454712\n",
      "Loss: 3.804600715637207\n",
      "Loss: 3.4390006065368652\n",
      "Loss: 4.045257568359375\n",
      "Loss: 3.49261212348938\n",
      "Loss: 3.5875096321105957\n",
      "Loss: 3.6811838150024414\n",
      "Loss: 3.600679874420166\n",
      "Loss: 3.4979474544525146\n",
      "Loss: 3.6492903232574463\n",
      "Loss: 3.6247122287750244\n",
      "Loss: 3.47173810005188\n",
      "Loss: 3.604642868041992\n",
      "Loss: 3.5899546146392822\n",
      "Loss: 3.761143445968628\n",
      "Loss: 3.67984676361084\n",
      "Loss: 3.817456007003784\n",
      "Loss: 3.581359624862671\n",
      "Loss: 3.220057249069214\n",
      "Loss: 3.440082311630249\n",
      "Loss: 3.525468111038208\n",
      "Loss: 3.657491683959961\n",
      "Loss: 3.576481342315674\n",
      "Loss: 3.5809247493743896\n",
      "Loss: 3.7770628929138184\n",
      "Loss: 3.68520188331604\n",
      "Loss: 3.29677152633667\n",
      "Loss: 3.536677837371826\n",
      "Loss: 3.632023811340332\n",
      "Loss: 3.6716554164886475\n",
      "Loss: 3.6441385746002197\n",
      "Loss: 3.6357545852661133\n",
      "Loss: 3.5935332775115967\n",
      "Loss: 3.655940532684326\n",
      "Loss: 3.529235363006592\n",
      "Loss: 3.6340348720550537\n",
      "Loss: 3.684966564178467\n",
      "Loss: 3.4458136558532715\n",
      "Loss: 3.5213990211486816\n",
      "Loss: 3.6926722526550293\n",
      "Loss: 3.6191089153289795\n",
      "Loss: 3.462251901626587\n",
      "Loss: 3.682262420654297\n",
      "Loss: 3.7538986206054688\n",
      "Loss: 3.56790828704834\n",
      "Loss: 3.4439167976379395\n",
      "Loss: 3.6612603664398193\n",
      "Loss: 3.6455423831939697\n",
      "Loss: 3.538830518722534\n",
      "Loss: 3.5079596042633057\n",
      "Loss: 3.460571527481079\n",
      "Loss: 3.3822059631347656\n",
      "Loss: 3.614042282104492\n",
      "Loss: 3.740293502807617\n",
      "Loss: 3.5483241081237793\n",
      "Loss: 3.7223060131073\n",
      "Loss: 4.077340126037598\n",
      "Loss: 3.6287600994110107\n",
      "Loss: 3.6349093914031982\n",
      "Loss: 3.5835981369018555\n",
      "Loss: 3.6436362266540527\n",
      "Loss: 3.51318097114563\n",
      "Loss: 3.512720823287964\n",
      "Loss: 3.4840540885925293\n",
      "Loss: 3.5783557891845703\n",
      "Loss: 3.4939968585968018\n",
      "Loss: 3.625687599182129\n",
      "Loss: 3.579216241836548\n",
      "Loss: 3.4843802452087402\n",
      "Loss: 3.6989212036132812\n",
      "Loss: 3.5763916969299316\n",
      "Loss: 3.5092434883117676\n",
      "Loss: 3.8630425930023193\n",
      "Loss: 3.863370418548584\n",
      "Loss: 3.751746416091919\n",
      "Loss: 3.406510829925537\n",
      "Loss: 3.727734327316284\n",
      "Loss: 3.349315881729126\n",
      "Loss: 3.7510335445404053\n",
      "Loss: 3.606771469116211\n",
      "Loss: 3.2476696968078613\n",
      "Loss: 3.4686665534973145\n",
      "Loss: 3.2782328128814697\n",
      "Loss: 3.8333730697631836\n",
      "Loss: 3.615727186203003\n",
      "Loss: 3.83308744430542\n",
      "Loss: 3.5091440677642822\n",
      "Loss: 3.5732879638671875\n",
      "Loss: 3.599719285964966\n",
      "Loss: 3.684736728668213\n",
      "Loss: 3.6673550605773926\n",
      "Loss: 3.7114644050598145\n",
      "Loss: 3.719386100769043\n",
      "Loss: 3.349501132965088\n",
      "Loss: 3.7076423168182373\n",
      "Loss: 3.4480409622192383\n",
      "Loss: 3.6300694942474365\n",
      "Loss: 3.644878625869751\n",
      "Loss: 3.6748852729797363\n",
      "Loss: 3.6206092834472656\n",
      "Loss: 3.643981695175171\n",
      "Loss: 3.3189539909362793\n",
      "Loss: 3.704275131225586\n",
      "Loss: 3.708514451980591\n",
      "Loss: 3.654623031616211\n",
      "Loss: 3.3864855766296387\n",
      "Loss: 3.4814751148223877\n",
      "Loss: 3.732913017272949\n",
      "Loss: 3.6947319507598877\n",
      "Loss: 3.500368118286133\n",
      "Loss: 3.7158591747283936\n",
      "Loss: 3.529623508453369\n",
      "Loss: 3.881162166595459\n",
      "Loss: 3.614729642868042\n",
      "Loss: 3.420461416244507\n",
      "Loss: 3.3012914657592773\n",
      "Loss: 3.5777907371520996\n",
      "Loss: 3.8044497966766357\n",
      "Loss: 3.3769898414611816\n",
      "Loss: 3.569732904434204\n",
      "Loss: 3.6101691722869873\n",
      "Loss: 3.5720748901367188\n",
      "Loss: 3.7943460941314697\n",
      "Loss: 3.5662529468536377\n",
      "Loss: 3.669992446899414\n",
      "Loss: 3.4379732608795166\n",
      "Loss: 3.2560222148895264\n",
      "Loss: 3.489999294281006\n",
      "Loss: 3.6689558029174805\n",
      "Loss: 3.4838991165161133\n",
      "Loss: 3.8055543899536133\n",
      "Loss: 3.482083320617676\n",
      "Loss: 3.486966371536255\n",
      "Loss: 3.4491209983825684\n",
      "Loss: 3.4958465099334717\n",
      "Loss: 3.773483991622925\n",
      "Loss: 3.6652419567108154\n",
      "Loss: 3.513584852218628\n",
      "Loss: 3.558551073074341\n",
      "Loss: 3.1525704860687256\n",
      "Loss: 3.682173728942871\n",
      "Loss: 3.377910852432251\n",
      "Loss: 3.756930351257324\n",
      "Loss: 3.52923321723938\n",
      "Loss: 3.5256550312042236\n",
      "Loss: 3.522218942642212\n",
      "Loss: 3.6635303497314453\n",
      "Loss: 3.5365066528320312\n",
      "Loss: 3.4869627952575684\n",
      "Loss: 3.2155165672302246\n",
      "Loss: 3.4990477561950684\n",
      "Loss: 3.559452772140503\n",
      "Loss: 3.7229561805725098\n",
      "Loss: 3.4501590728759766\n",
      "Loss: 3.500140428543091\n",
      "Loss: 3.565612316131592\n",
      "Loss: 3.4644582271575928\n",
      "Loss: 3.5091850757598877\n",
      "Loss: 3.2290403842926025\n",
      "Loss: 3.313067674636841\n",
      "Loss: 3.728654623031616\n",
      "Loss: 3.4283125400543213\n",
      "Loss: 3.5072038173675537\n",
      "Loss: 3.745420217514038\n",
      "Loss: 3.490365743637085\n",
      "Loss: 3.5258443355560303\n",
      "Loss: 3.6725075244903564\n",
      "Loss: 3.484908103942871\n",
      "Loss: 3.5163145065307617\n",
      "Loss: 3.9449870586395264\n",
      "Loss: 3.5914669036865234\n",
      "Loss: 3.6839702129364014\n",
      "Loss: 3.3597543239593506\n",
      "Loss: 3.5806679725646973\n",
      "Loss: 3.6321725845336914\n",
      "Loss: 3.65708327293396\n",
      "Loss: 3.6509881019592285\n",
      "Loss: 3.2787139415740967\n",
      "Loss: 3.1990606784820557\n",
      "Loss: 3.460263252258301\n",
      "Loss: 3.5896942615509033\n",
      "Loss: 3.4737868309020996\n",
      "Loss: 3.5531182289123535\n",
      "Loss: 3.6417086124420166\n",
      "Loss: 3.6253371238708496\n",
      "Loss: 3.4278831481933594\n",
      "Loss: 3.5964772701263428\n",
      "Loss: 3.31992769241333\n",
      "Loss: 3.718137264251709\n",
      "Loss: 3.5456278324127197\n",
      "Loss: 3.612596035003662\n",
      "Loss: 3.4517643451690674\n",
      "Loss: 3.5495643615722656\n",
      "Loss: 3.3430304527282715\n",
      "Loss: 3.4777114391326904\n",
      "Loss: 3.4296255111694336\n",
      "Loss: 3.5131113529205322\n",
      "Loss: 3.4122838973999023\n",
      "Loss: 3.356139898300171\n",
      "Loss: 3.5228497982025146\n",
      "Loss: 3.578491449356079\n",
      "Loss: 3.773216724395752\n",
      "Loss: 3.448068380355835\n",
      "Loss: 3.6762564182281494\n",
      "Loss: 3.4845380783081055\n",
      "Loss: 3.547666072845459\n",
      "Loss: 3.530402421951294\n",
      "Loss: 3.7067670822143555\n",
      "Loss: 3.687377452850342\n",
      "Loss: 3.5751993656158447\n",
      "Loss: 3.3770482540130615\n",
      "Loss: 3.5241146087646484\n",
      "Loss: 3.5463497638702393\n",
      "Loss: 3.7644705772399902\n",
      "Loss: 3.4673919677734375\n",
      "Loss: 3.356431245803833\n",
      "Loss: 3.4165024757385254\n",
      "Loss: 3.47599458694458\n",
      "Loss: 3.591442346572876\n",
      "Loss: 3.7262539863586426\n",
      "Loss: 3.4144768714904785\n",
      "Loss: 3.337085008621216\n",
      "Loss: 3.349518060684204\n",
      "Loss: 3.3250436782836914\n",
      "Loss: 3.4990859031677246\n",
      "Loss: 3.307810068130493\n",
      "Loss: 3.516998291015625\n",
      "Loss: 3.568936586380005\n",
      "Loss: 3.338038206100464\n",
      "Loss: 3.4373297691345215\n",
      "Loss: 3.3586456775665283\n",
      "Loss: 3.551677942276001\n",
      "Loss: 3.721900463104248\n",
      "Loss: 3.332430601119995\n",
      "Loss: 3.5727202892303467\n",
      "Loss: 3.282949924468994\n",
      "Loss: 3.3416988849639893\n",
      "Loss: 3.582698106765747\n",
      "Loss: 3.509549617767334\n",
      "Loss: 3.4995529651641846\n",
      "Loss: 3.400148391723633\n",
      "Loss: 3.183117628097534\n",
      "Loss: 3.6043643951416016\n",
      "Loss: 3.650470018386841\n",
      "Loss: 3.493455648422241\n",
      "Loss: 3.4134178161621094\n",
      "Loss: 3.2315502166748047\n",
      "Loss: 3.265408515930176\n",
      "Loss: 3.442678689956665\n",
      "Loss: 3.4327046871185303\n",
      "Loss: 3.8016698360443115\n",
      "Loss: 3.534754991531372\n",
      "Loss: 3.380643367767334\n",
      "Loss: 3.52053165435791\n",
      "Loss: 3.460428476333618\n",
      "Loss: 3.4400076866149902\n",
      "Loss: 3.7104973793029785\n",
      "Loss: 3.2852230072021484\n",
      "Loss: 3.25376558303833\n",
      "Loss: 3.253732442855835\n",
      "Loss: 3.6020336151123047\n",
      "Loss: 3.526813268661499\n",
      "Loss: 3.5640456676483154\n",
      "Loss: 3.3726589679718018\n",
      "Loss: 3.564714193344116\n",
      "Loss: 3.1733243465423584\n",
      "Loss: 3.5957446098327637\n",
      "Loss: 3.7057735919952393\n",
      "Loss: 3.621192693710327\n",
      "Loss: 3.4071202278137207\n",
      "Loss: 3.2657008171081543\n",
      "Loss: 3.291630506515503\n",
      "Loss: 3.57525372505188\n",
      "Loss: 3.0946028232574463\n",
      "Loss: 3.4735705852508545\n",
      "Loss: 3.616736650466919\n",
      "Loss: 3.548107385635376\n",
      "Loss: 3.262622594833374\n",
      "Loss: 3.5090172290802\n",
      "Loss: 3.375889778137207\n",
      "Loss: 3.4851362705230713\n",
      "Loss: 3.4672086238861084\n",
      "Loss: 3.38284969329834\n",
      "Loss: 3.5629072189331055\n",
      "Loss: 3.281322479248047\n",
      "Loss: 3.4937212467193604\n",
      "Loss: 3.4479176998138428\n",
      "Loss: 3.5734665393829346\n",
      "Loss: 3.690915584564209\n",
      "Loss: 3.44636869430542\n",
      "Loss: 3.3841779232025146\n",
      "Loss: 3.5163679122924805\n",
      "Loss: 3.3980133533477783\n",
      "Loss: 3.4614903926849365\n",
      "Loss: 3.3733067512512207\n",
      "Loss: 3.7007522583007812\n",
      "Loss: 3.358020544052124\n",
      "Loss: 3.1931450366973877\n",
      "Loss: 3.7553775310516357\n",
      "Loss: 3.483903408050537\n",
      "Loss: 3.5236804485321045\n",
      "Loss: 3.71059250831604\n",
      "Loss: 3.2600650787353516\n",
      "Loss: 3.3667850494384766\n",
      "Loss: 3.359156847000122\n",
      "Loss: 3.5536773204803467\n",
      "Loss: 3.4591503143310547\n",
      "Loss: 3.743150472640991\n",
      "Loss: 3.382502555847168\n",
      "Loss: 3.4090754985809326\n",
      "Loss: 3.605466842651367\n",
      "Loss: 3.3573248386383057\n",
      "Loss: 3.623703718185425\n",
      "Loss: 3.459662675857544\n",
      "Loss: 3.3988661766052246\n",
      "Loss: 3.4321632385253906\n",
      "Loss: 3.2915661334991455\n",
      "Loss: 3.2965550422668457\n",
      "Loss: 3.4668285846710205\n",
      "Loss: 3.5518009662628174\n",
      "Loss: 3.403686285018921\n",
      "Loss: 3.362670421600342\n",
      "Loss: 3.080946922302246\n",
      "Loss: 3.3878369331359863\n",
      "Loss: 3.2361817359924316\n",
      "Loss: 3.3021886348724365\n",
      "Loss: 3.460526466369629\n",
      "Loss: 3.5349507331848145\n",
      "Loss: 3.79052472114563\n",
      "Loss: 3.4537131786346436\n",
      "Loss: 3.4849679470062256\n",
      "Loss: 3.429673433303833\n",
      "Loss: 3.2007110118865967\n",
      "Loss: 3.1992058753967285\n",
      "Loss: 3.4115617275238037\n",
      "Loss: 3.3511767387390137\n",
      "Loss: 3.542670726776123\n",
      "Loss: 3.5047526359558105\n",
      "Loss: 3.761718273162842\n",
      "Loss: 3.4174892902374268\n",
      "Loss: 3.240715742111206\n",
      "Loss: 3.4330711364746094\n",
      "Loss: 3.4393417835235596\n",
      "Loss: 3.7265114784240723\n",
      "Loss: 3.356386184692383\n",
      "Loss: 3.1910531520843506\n",
      "Loss: 3.2356505393981934\n",
      "Loss: 3.469558000564575\n",
      "Loss: 3.56473708152771\n",
      "Loss: 3.2726974487304688\n",
      "Loss: 3.509068489074707\n",
      "Loss: 3.432711124420166\n",
      "Loss: 3.233825445175171\n",
      "Loss: 3.547323703765869\n",
      "Loss: 3.3731637001037598\n",
      "Loss: 3.20428204536438\n",
      "Loss: 3.4841995239257812\n",
      "Loss: 3.2843964099884033\n",
      "Loss: 3.3691751956939697\n",
      "Loss: 3.2272355556488037\n",
      "Loss: 3.512385845184326\n",
      "Loss: 3.3167693614959717\n",
      "Loss: 3.716387987136841\n",
      "Loss: 3.4681906700134277\n",
      "Loss: 3.543686866760254\n",
      "Loss: 3.267899513244629\n",
      "Loss: 3.450397491455078\n",
      "Loss: 3.661543369293213\n",
      "Loss: 3.6184775829315186\n",
      "Loss: 3.5967512130737305\n",
      "Loss: 3.329749345779419\n",
      "Loss: 3.9040582180023193\n",
      "Loss: 3.3968734741210938\n",
      "Loss: 3.3156492710113525\n",
      "Loss: 2.9654934406280518\n",
      "Loss: 3.415534734725952\n",
      "Loss: 3.381840467453003\n",
      "Loss: 3.4388957023620605\n",
      "Loss: 3.148540496826172\n",
      "Loss: 3.3638432025909424\n",
      "Loss: 3.457298994064331\n",
      "Loss: 3.564359426498413\n",
      "Loss: 3.604335069656372\n",
      "Loss: 3.6269161701202393\n",
      "Loss: 3.610292434692383\n",
      "Loss: 3.4409239292144775\n",
      "Loss: 3.7010884284973145\n",
      "Loss: 3.8810980319976807\n",
      "Loss: 3.7004129886627197\n",
      "Loss: 3.370753765106201\n",
      "Loss: 3.4636948108673096\n",
      "Loss: 3.6666717529296875\n",
      "Loss: 3.6477420330047607\n",
      "Loss: 3.6969003677368164\n",
      "Loss: 3.6151750087738037\n",
      "Loss: 3.8698251247406006\n",
      "Loss: 3.6434547901153564\n",
      "Loss: 3.5707590579986572\n",
      "Loss: 3.567897081375122\n",
      "Loss: 3.5289523601531982\n",
      "Loss: 3.464430809020996\n",
      "Loss: 3.796431303024292\n",
      "Loss: 3.444366931915283\n",
      "Loss: 3.5849223136901855\n",
      "Loss: 3.677767753601074\n",
      "Loss: 3.5823848247528076\n",
      "Loss: 3.5536234378814697\n",
      "Loss: 3.6091787815093994\n",
      "Loss: 3.591714859008789\n",
      "Loss: 3.5848991870880127\n",
      "Loss: 3.709397554397583\n",
      "Loss: 3.3769021034240723\n",
      "Loss: 3.5426013469696045\n",
      "Loss: 3.482224464416504\n",
      "Loss: 3.472527027130127\n",
      "Loss: 3.7263071537017822\n",
      "Loss: 3.678569793701172\n",
      "Loss: 3.7194340229034424\n",
      "Loss: 3.8751206398010254\n",
      "Loss: 3.544250249862671\n",
      "Loss: 3.719057559967041\n",
      "Loss: 3.567664384841919\n",
      "Loss: 3.3800947666168213\n",
      "Loss: 3.7259764671325684\n",
      "Loss: 3.7543866634368896\n",
      "Loss: 3.38663649559021\n",
      "Loss: 3.680619478225708\n",
      "Loss: 3.703681230545044\n",
      "Loss: 3.7697126865386963\n",
      "Loss: 3.410069465637207\n",
      "Loss: 4.013373851776123\n",
      "Loss: 3.7208411693573\n",
      "Loss: 3.4688010215759277\n",
      "Loss: 3.460089683532715\n",
      "Loss: 3.615893602371216\n",
      "Loss: 3.678330659866333\n",
      "Loss: 3.415073871612549\n",
      "Loss: 3.557034492492676\n",
      "Loss: 3.759018659591675\n",
      "Loss: 3.588508129119873\n",
      "Loss: 3.427626371383667\n",
      "Loss: 3.381740093231201\n",
      "Loss: 3.481252431869507\n",
      "Loss: 3.6938607692718506\n",
      "Loss: 3.6481966972351074\n",
      "Loss: 3.78942608833313\n",
      "Loss: 3.630920648574829\n",
      "Loss: 3.855961799621582\n",
      "Loss: 3.425429105758667\n",
      "Loss: 3.408449649810791\n",
      "Loss: 3.5497961044311523\n",
      "Loss: 3.5881834030151367\n",
      "Loss: 3.554297685623169\n",
      "Loss: 3.5717334747314453\n",
      "Loss: 3.7679250240325928\n",
      "Loss: 3.5401902198791504\n",
      "Loss: 3.3912770748138428\n",
      "Loss: 3.632397174835205\n",
      "Loss: 3.5520670413970947\n",
      "Loss: 3.487837314605713\n",
      "Loss: 3.4119856357574463\n",
      "Loss: 3.531789541244507\n",
      "Loss: 3.359571933746338\n",
      "Loss: 3.7342441082000732\n",
      "Loss: 3.4214582443237305\n",
      "Loss: 3.433589220046997\n",
      "Loss: 3.434321403503418\n",
      "Loss: 3.777365207672119\n",
      "Loss: 3.3706698417663574\n",
      "Loss: 3.4641313552856445\n",
      "Loss: 3.416905403137207\n",
      "Loss: 3.6744468212127686\n",
      "Loss: 3.596473217010498\n",
      "Loss: 3.692591667175293\n",
      "Loss: 3.6670355796813965\n",
      "Loss: 3.572484254837036\n",
      "Loss: 3.5452661514282227\n",
      "Loss: 3.3733224868774414\n",
      "Loss: 3.4877727031707764\n",
      "Loss: 3.395282030105591\n",
      "Loss: 3.622152090072632\n",
      "Loss: 3.3537521362304688\n",
      "Loss: 3.9945054054260254\n",
      "Loss: 3.4129161834716797\n",
      "Loss: 3.3715524673461914\n",
      "Loss: 3.455080032348633\n",
      "Loss: 3.7322404384613037\n",
      "Loss: 3.3587353229522705\n",
      "Loss: 3.476275682449341\n",
      "Loss: 3.443544387817383\n",
      "Loss: 3.6528775691986084\n",
      "Loss: 3.913452625274658\n",
      "Loss: 3.6855626106262207\n",
      "Loss: 3.5868444442749023\n",
      "Loss: 3.5648136138916016\n",
      "Loss: 3.5692925453186035\n",
      "Loss: 3.421959638595581\n",
      "Loss: 3.4637484550476074\n",
      "Loss: 3.845010995864868\n",
      "Loss: 3.5964086055755615\n",
      "Loss: 3.877997398376465\n",
      "Loss: 3.439122438430786\n",
      "Loss: 3.611032247543335\n",
      "Loss: 3.3809444904327393\n",
      "Loss: 3.514803171157837\n",
      "Loss: 3.7830123901367188\n",
      "Loss: 3.8503661155700684\n",
      "Loss: 3.597256898880005\n",
      "Loss: 3.514749765396118\n",
      "Loss: 3.7270960807800293\n",
      "Loss: 3.5500035285949707\n",
      "Loss: 3.7478697299957275\n",
      "Loss: 3.4227535724639893\n",
      "Loss: 3.4538936614990234\n",
      "Loss: 3.961970329284668\n",
      "Loss: 3.5737905502319336\n",
      "Loss: 4.011213302612305\n",
      "Loss: 3.5061709880828857\n",
      "Loss: 3.441981077194214\n",
      "Loss: 3.565769672393799\n",
      "Loss: 3.843961000442505\n",
      "Loss: 3.6757707595825195\n",
      "Loss: 3.218207359313965\n",
      "Loss: 3.6330480575561523\n",
      "Loss: 3.553389072418213\n",
      "Loss: 3.7801289558410645\n",
      "Loss: 3.43806529045105\n",
      "Loss: 3.620965003967285\n",
      "Loss: 3.502591371536255\n",
      "Loss: 3.1790711879730225\n",
      "Loss: 3.9615187644958496\n",
      "Loss: 3.6167151927948\n",
      "Loss: 3.8947155475616455\n",
      "Loss: 3.456275701522827\n",
      "Loss: 3.1999642848968506\n",
      "Loss: 3.574589729309082\n",
      "Loss: 3.3562846183776855\n",
      "Loss: 3.525602340698242\n",
      "Loss: 3.397616386413574\n",
      "Loss: 3.3738656044006348\n",
      "Loss: 3.4366097450256348\n",
      "Loss: 3.673969030380249\n",
      "Loss: 3.386582851409912\n",
      "Loss: 3.357893705368042\n",
      "Loss: 3.6335830688476562\n",
      "Loss: 3.5482845306396484\n",
      "Loss: 3.523984432220459\n",
      "Loss: 3.174779176712036\n",
      "Loss: 3.134376049041748\n",
      "Loss: 3.7206099033355713\n",
      "Loss: 3.8970448970794678\n",
      "Loss: 3.6730504035949707\n",
      "Loss: 3.322690725326538\n",
      "Loss: 3.4863407611846924\n",
      "Loss: 3.6003847122192383\n",
      "Loss: 3.4539482593536377\n",
      "Loss: 3.640389919281006\n",
      "Loss: 3.3204240798950195\n",
      "Loss: 3.4384684562683105\n",
      "Loss: 3.517496109008789\n",
      "Loss: 3.4036238193511963\n",
      "Loss: 3.433215856552124\n",
      "Loss: 3.5428051948547363\n",
      "Loss: 3.424236536026001\n",
      "Loss: 3.5779974460601807\n",
      "Loss: 4.057094573974609\n",
      "Loss: 3.102982521057129\n",
      "Loss: 3.4050614833831787\n",
      "Loss: 3.5226244926452637\n",
      "Loss: 3.6747536659240723\n",
      "Loss: 3.5777597427368164\n",
      "Loss: 3.423173427581787\n",
      "Loss: 3.8022680282592773\n",
      "Loss: 3.5770621299743652\n",
      "Loss: 3.5392918586730957\n",
      "Loss: 3.5047905445098877\n",
      "Loss: 3.484769821166992\n",
      "Loss: 3.4049575328826904\n",
      "Loss: 3.4869163036346436\n",
      "Loss: 3.2843711376190186\n",
      "Loss: 3.440016984939575\n",
      "Loss: 3.450263261795044\n",
      "Loss: 3.6233842372894287\n",
      "Loss: 3.5826096534729004\n",
      "Loss: 3.5258612632751465\n",
      "Loss: 3.4864084720611572\n",
      "Loss: 3.5066962242126465\n",
      "Loss: 3.5997936725616455\n",
      "Loss: 3.4415557384490967\n",
      "Loss: 3.512371301651001\n",
      "Loss: 3.7849631309509277\n",
      "Loss: 3.6655380725860596\n",
      "Loss: 3.6789028644561768\n",
      "Loss: 3.678220510482788\n",
      "Loss: 3.378770589828491\n",
      "Loss: 3.771934986114502\n",
      "Loss: 3.295825719833374\n",
      "Loss: 3.4264962673187256\n",
      "Loss: 3.9453542232513428\n",
      "Loss: 3.5549697875976562\n",
      "Loss: 3.4166996479034424\n",
      "Loss: 3.2838878631591797\n",
      "Loss: 3.6255180835723877\n",
      "Loss: 3.6988072395324707\n",
      "Loss: 3.5035202503204346\n",
      "Loss: 3.6859920024871826\n",
      "Loss: 3.2877182960510254\n",
      "Loss: 3.335580348968506\n",
      "Loss: 3.601067304611206\n",
      "Loss: 3.449970245361328\n",
      "Loss: 3.4186670780181885\n",
      "Loss: 3.3532979488372803\n",
      "Loss: 3.6866655349731445\n",
      "Loss: 3.1997146606445312\n",
      "Loss: 3.5796895027160645\n",
      "Loss: 3.635746717453003\n",
      "Loss: 3.5487477779388428\n",
      "Loss: 3.6387462615966797\n",
      "Loss: 3.412926435470581\n",
      "Loss: 3.5094351768493652\n",
      "Loss: 3.33479380607605\n",
      "Loss: 3.701106071472168\n",
      "Loss: 3.4000515937805176\n",
      "Loss: 3.7032487392425537\n",
      "Loss: 3.525400161743164\n",
      "Loss: 3.660460948944092\n",
      "Loss: 3.505385637283325\n",
      "Loss: 3.7892396450042725\n",
      "Loss: 3.383200168609619\n",
      "Loss: 3.465886116027832\n",
      "Loss: 3.3450489044189453\n",
      "Loss: 3.4563777446746826\n",
      "Loss: 3.2422144412994385\n",
      "Loss: 3.5863633155822754\n",
      "Loss: 3.4787495136260986\n",
      "Loss: 3.3496556282043457\n",
      "Loss: 3.385571241378784\n",
      "Loss: 3.4384074211120605\n",
      "Loss: 3.3380608558654785\n",
      "Loss: 3.590679407119751\n",
      "Loss: 3.009085178375244\n",
      "Loss: 3.4842188358306885\n",
      "Loss: 3.431410074234009\n",
      "Loss: 3.655247211456299\n",
      "Loss: 3.272310733795166\n",
      "Loss: 3.370492935180664\n",
      "Loss: 3.8046674728393555\n",
      "Loss: 3.3633246421813965\n",
      "Loss: 3.4228146076202393\n",
      "Loss: 3.449904441833496\n",
      "Loss: 3.5615615844726562\n",
      "Loss: 3.4637136459350586\n",
      "Loss: 3.599039316177368\n",
      "Loss: 3.6016488075256348\n",
      "Loss: 3.571221113204956\n",
      "Loss: 3.393507242202759\n",
      "Loss: 3.3254973888397217\n",
      "Loss: 3.6853420734405518\n",
      "Loss: 3.669318199157715\n",
      "Loss: 3.36081862449646\n",
      "Loss: 3.873242139816284\n",
      "Loss: 3.497695207595825\n",
      "Loss: 3.5477802753448486\n",
      "Loss: 3.6938931941986084\n",
      "Loss: 3.4920272827148438\n",
      "Loss: 3.4241585731506348\n",
      "Loss: 3.7201004028320312\n",
      "Loss: 3.018568515777588\n",
      "Loss: 3.0916178226470947\n",
      "Loss: 3.6070566177368164\n",
      "Loss: 3.570213556289673\n",
      "Loss: 3.6916611194610596\n",
      "Loss: 3.364116907119751\n",
      "Loss: 3.5811052322387695\n",
      "Loss: 3.3424675464630127\n",
      "Loss: 3.580967903137207\n",
      "Loss: 3.3381190299987793\n",
      "Loss: 3.7329659461975098\n",
      "Loss: 3.434150218963623\n",
      "Loss: 3.660417318344116\n",
      "Loss: 3.3841824531555176\n",
      "Loss: 3.3986706733703613\n",
      "Loss: 3.405027151107788\n",
      "Loss: 3.2588706016540527\n",
      "Loss: 3.395244598388672\n",
      "Loss: 3.557194948196411\n",
      "Loss: 3.3535616397857666\n",
      "Loss: 3.7475011348724365\n",
      "Loss: 3.4279985427856445\n",
      "Loss: 3.653236150741577\n",
      "Loss: 3.3878989219665527\n",
      "Loss: 3.622830390930176\n",
      "Loss: 3.367161989212036\n",
      "Loss: 3.295278549194336\n",
      "Loss: 3.6417083740234375\n",
      "Loss: 3.605313539505005\n",
      "Loss: 3.6383607387542725\n",
      "Loss: 3.5060787200927734\n",
      "Loss: 3.456047296524048\n",
      "Loss: 3.4629712104797363\n",
      "Loss: 3.3799304962158203\n",
      "Loss: 3.6776275634765625\n",
      "Loss: 3.7359509468078613\n",
      "Loss: 3.2217092514038086\n",
      "Loss: 3.5605568885803223\n",
      "Loss: 3.654153347015381\n",
      "Loss: 3.502028226852417\n",
      "Loss: 3.2847366333007812\n",
      "Loss: 3.4340641498565674\n",
      "Loss: 3.6528513431549072\n",
      "Loss: 3.3010005950927734\n",
      "Loss: 3.244072437286377\n",
      "Loss: 3.543635606765747\n",
      "Loss: 3.152500867843628\n",
      "Loss: 3.2089192867279053\n",
      "Loss: 3.9182982444763184\n",
      "Loss: 3.3016529083251953\n",
      "Loss: 3.4821884632110596\n",
      "Loss: 3.2935338020324707\n",
      "Loss: 4.00353479385376\n",
      "Loss: 3.4292447566986084\n",
      "Loss: 3.354987382888794\n",
      "Loss: 3.281602621078491\n",
      "Loss: 3.5070712566375732\n",
      "Loss: 3.445760488510132\n",
      "Loss: 3.277470111846924\n",
      "Loss: 3.4584884643554688\n",
      "Loss: 3.207186698913574\n",
      "Loss: 3.3704066276550293\n",
      "Loss: 3.665276527404785\n",
      "Loss: 3.5350890159606934\n",
      "Loss: 2.9238977432250977\n",
      "Loss: 3.5653393268585205\n",
      "Loss: 3.375894784927368\n",
      "Loss: 3.3960623741149902\n",
      "Loss: 3.267878532409668\n",
      "Loss: 3.2556192874908447\n",
      "Loss: 3.628472089767456\n",
      "Loss: 3.1933815479278564\n",
      "Loss: 3.5402045249938965\n",
      "Loss: 3.363680362701416\n",
      "Loss: 3.4953057765960693\n",
      "Loss: 3.2235500812530518\n",
      "Loss: 3.934389114379883\n",
      "Loss: 3.1910552978515625\n",
      "Loss: 3.311389923095703\n",
      "Loss: 3.417208194732666\n",
      "Loss: 3.4560530185699463\n",
      "Loss: 3.42087984085083\n",
      "Loss: 3.38023042678833\n",
      "Loss: 3.206040382385254\n",
      "Loss: 3.539661169052124\n",
      "Loss: 3.6497185230255127\n",
      "Loss: 3.625004529953003\n",
      "Loss: 3.5682263374328613\n",
      "Loss: 3.4299099445343018\n",
      "Loss: 3.571078062057495\n",
      "Loss: 3.405031681060791\n",
      "Loss: 3.2654449939727783\n",
      "Loss: 3.273674249649048\n",
      "Loss: 3.2174925804138184\n",
      "Loss: 3.408310651779175\n",
      "Loss: 3.302765130996704\n",
      "Loss: 3.508275270462036\n",
      "Loss: 3.571535348892212\n",
      "Loss: 3.2894973754882812\n",
      "Loss: 3.3158860206604004\n",
      "Loss: 3.581829071044922\n",
      "Loss: 3.532266855239868\n",
      "Loss: 3.4507932662963867\n",
      "Loss: 3.3015832901000977\n",
      "Loss: 3.4440367221832275\n",
      "Loss: 3.3571832180023193\n",
      "Loss: 3.5458853244781494\n",
      "Loss: 3.7182517051696777\n",
      "Loss: 3.3222439289093018\n",
      "Loss: 3.612241268157959\n",
      "Loss: 3.4528648853302\n",
      "Loss: 3.2273476123809814\n",
      "Loss: 3.288013458251953\n",
      "Loss: 3.4056589603424072\n",
      "Loss: 3.2959518432617188\n",
      "Loss: 3.2561705112457275\n",
      "Loss: 3.0494132041931152\n",
      "Loss: 3.23964524269104\n",
      "Loss: 3.2221720218658447\n",
      "Loss: 3.5581820011138916\n",
      "Loss: 3.6474244594573975\n",
      "Loss: 3.355417013168335\n",
      "Loss: 3.7380001544952393\n",
      "Loss: 3.539553642272949\n",
      "Loss: 3.3306729793548584\n",
      "Loss: 3.609548330307007\n",
      "Loss: 3.4706382751464844\n",
      "Loss: 3.4655637741088867\n",
      "Loss: 3.1891603469848633\n",
      "Loss: 3.4406208992004395\n",
      "Loss: 3.473728656768799\n",
      "Loss: 3.6318860054016113\n",
      "Loss: 3.5125792026519775\n",
      "Loss: 3.3507399559020996\n",
      "Loss: 3.361539602279663\n",
      "Loss: 3.1361823081970215\n",
      "Loss: 3.7134711742401123\n",
      "Loss: 3.321485757827759\n",
      "Loss: 3.5260133743286133\n",
      "Loss: 3.289767265319824\n",
      "Loss: 3.2048449516296387\n",
      "Loss: 3.3073348999023438\n",
      "Loss: 3.306138753890991\n",
      "Loss: 3.0644121170043945\n",
      "Loss: 3.499013900756836\n",
      "Loss: 3.3778436183929443\n",
      "Loss: 3.48771333694458\n",
      "Loss: 3.4965484142303467\n",
      "Loss: 3.395057201385498\n",
      "Loss: 3.265852212905884\n",
      "Loss: 3.244555711746216\n",
      "Loss: 3.672771692276001\n",
      "Loss: 3.196702480316162\n",
      "Loss: 3.3451383113861084\n",
      "Loss: 3.233685255050659\n",
      "Loss: 3.075801372528076\n",
      "Loss: 3.4696836471557617\n",
      "Loss: 3.8359005451202393\n",
      "Loss: 3.640277147293091\n",
      "Loss: 3.6963067054748535\n",
      "Loss: 3.2124099731445312\n",
      "Loss: 3.222289562225342\n",
      "Loss: 3.4438869953155518\n",
      "Loss: 3.5186092853546143\n",
      "Loss: 3.5161571502685547\n",
      "Loss: 3.448742151260376\n",
      "Loss: 3.199908494949341\n",
      "Loss: 3.1856749057769775\n",
      "Loss: 3.222097158432007\n",
      "Loss: 3.3209333419799805\n",
      "Loss: 3.3042807579040527\n",
      "Loss: 3.618600606918335\n",
      "Loss: 3.6943016052246094\n",
      "Loss: 3.5651357173919678\n",
      "Loss: 3.7332582473754883\n",
      "Loss: 3.359037399291992\n",
      "Loss: 3.5128681659698486\n",
      "Loss: 3.5629868507385254\n",
      "Loss: 3.228065013885498\n",
      "Loss: 3.3964056968688965\n",
      "Loss: 3.409374713897705\n",
      "Loss: 3.5798096656799316\n",
      "Loss: 3.550482749938965\n",
      "Loss: 3.2566282749176025\n",
      "Loss: 3.35410737991333\n",
      "Loss: 3.387789249420166\n",
      "Loss: 3.5135676860809326\n",
      "Loss: 3.3890624046325684\n",
      "Loss: 3.2321457862854004\n",
      "Loss: 3.3662362098693848\n",
      "Loss: 3.333420991897583\n",
      "Loss: 3.2387073040008545\n",
      "Loss: 3.5566203594207764\n",
      "Loss: 3.441106081008911\n",
      "Loss: 3.226517677307129\n",
      "Loss: 3.556016683578491\n",
      "Loss: 3.458699941635132\n",
      "Loss: 3.322498083114624\n",
      "Loss: 3.4735264778137207\n",
      "Loss: 3.6649606227874756\n",
      "Loss: 3.2068629264831543\n",
      "Loss: 3.4293885231018066\n",
      "Loss: 3.1715948581695557\n",
      "Loss: 3.376185894012451\n",
      "Loss: 3.6194915771484375\n",
      "Loss: 3.3032262325286865\n",
      "Loss: 3.335550308227539\n",
      "Loss: 3.1014764308929443\n",
      "Loss: 3.5666685104370117\n",
      "Loss: 3.638197422027588\n",
      "Loss: 3.3268320560455322\n",
      "Loss: 3.4587888717651367\n",
      "Loss: 3.3319053649902344\n",
      "Loss: 3.2042112350463867\n",
      "Loss: 3.007694959640503\n",
      "Loss: 3.2682127952575684\n",
      "Loss: 3.3677072525024414\n",
      "Loss: 3.1911606788635254\n",
      "Loss: 3.474740505218506\n",
      "Loss: 3.36773419380188\n",
      "Loss: 3.278165340423584\n",
      "Loss: 3.251689910888672\n",
      "Loss: 3.5648674964904785\n",
      "Loss: 3.459043502807617\n",
      "Loss: 3.281093120574951\n",
      "Loss: 3.161989450454712\n",
      "Loss: 3.436600923538208\n",
      "Loss: 3.179441213607788\n",
      "Loss: 3.480638027191162\n",
      "Loss: 3.4692935943603516\n",
      "Loss: 3.7621042728424072\n",
      "Loss: 3.132136583328247\n",
      "Loss: 3.330411911010742\n",
      "Loss: 3.1269257068634033\n",
      "Loss: 3.2864108085632324\n",
      "Loss: 2.957815408706665\n",
      "Loss: 3.4214870929718018\n",
      "Loss: 3.1162989139556885\n",
      "Loss: 3.2235164642333984\n",
      "Loss: 3.3770956993103027\n",
      "Loss: 3.190340757369995\n",
      "Loss: 3.316378355026245\n",
      "Loss: 3.1091647148132324\n",
      "Loss: 3.070486307144165\n",
      "Loss: 3.2578282356262207\n",
      "Loss: 3.7306339740753174\n",
      "Loss: 3.6562254428863525\n",
      "Loss: 3.3636982440948486\n",
      "Loss: 3.155346393585205\n",
      "Loss: 3.630082130432129\n",
      "Loss: 3.6254491806030273\n",
      "Loss: 3.2725303173065186\n",
      "Loss: 3.5267467498779297\n",
      "Loss: 3.2717156410217285\n",
      "Loss: 3.3168182373046875\n",
      "Loss: 3.408174514770508\n",
      "Loss: 3.463684320449829\n",
      "Loss: 3.3452091217041016\n",
      "Loss: 3.560046434402466\n",
      "Loss: 3.617274761199951\n",
      "Loss: 3.452070474624634\n",
      "Loss: 3.2593204975128174\n",
      "Loss: 3.568340539932251\n",
      "Loss: 3.386056900024414\n",
      "Loss: 3.3447749614715576\n",
      "Loss: 3.2208969593048096\n",
      "Loss: 3.3267717361450195\n",
      "Loss: 3.2954325675964355\n",
      "Loss: 3.0004830360412598\n",
      "Loss: 3.350788116455078\n",
      "Loss: 3.181989908218384\n",
      "Loss: 3.289207696914673\n",
      "Loss: 3.2733442783355713\n",
      "Loss: 3.390782356262207\n",
      "Loss: 3.4311532974243164\n",
      "Loss: 3.3510220050811768\n",
      "Loss: 3.4268195629119873\n",
      "Loss: 3.27899169921875\n",
      "Loss: 3.3951964378356934\n",
      "Loss: 3.518419027328491\n",
      "Loss: 3.249619245529175\n",
      "Loss: 3.3741371631622314\n",
      "Loss: 3.577775478363037\n",
      "Loss: 3.400644302368164\n",
      "Loss: 3.153872489929199\n",
      "Loss: 3.2155869007110596\n",
      "Loss: 3.3980021476745605\n",
      "Loss: 3.0893545150756836\n",
      "Loss: 3.5594215393066406\n",
      "Loss: 3.4506072998046875\n",
      "Loss: 3.1239638328552246\n",
      "Loss: 3.2794275283813477\n",
      "Loss: 3.509342908859253\n",
      "Loss: 3.469956874847412\n",
      "Loss: 3.3650195598602295\n",
      "Loss: 3.2391445636749268\n",
      "Loss: 3.6715140342712402\n",
      "Loss: 3.21404767036438\n",
      "Loss: 3.4769396781921387\n",
      "Loss: 3.330934762954712\n",
      "Loss: 3.268983840942383\n",
      "Loss: 3.5207772254943848\n",
      "Loss: 3.25887393951416\n",
      "Loss: 3.2246477603912354\n",
      "Loss: 3.234072685241699\n",
      "Loss: 3.1692066192626953\n",
      "Loss: 3.4228687286376953\n",
      "Loss: 3.483881950378418\n",
      "Loss: 3.41171932220459\n",
      "Loss: 3.358618974685669\n",
      "Loss: 3.494375228881836\n",
      "Loss: 3.329044818878174\n",
      "Loss: 3.309844732284546\n",
      "Loss: 3.133091688156128\n",
      "Loss: 3.1955671310424805\n",
      "Loss: 3.44783878326416\n",
      "Loss: 3.4059243202209473\n",
      "Loss: 3.3171257972717285\n",
      "Loss: 3.3496575355529785\n",
      "Loss: 3.328796625137329\n",
      "Loss: 3.222775936126709\n",
      "Loss: 3.3313839435577393\n",
      "Loss: 3.2106454372406006\n",
      "Loss: 3.1751272678375244\n",
      "Loss: 3.590459108352661\n",
      "Loss: 3.3312883377075195\n",
      "Loss: 3.348816394805908\n",
      "Loss: 3.3824427127838135\n",
      "Loss: 3.3021998405456543\n",
      "Loss: 3.178847551345825\n",
      "Loss: 3.4905688762664795\n",
      "Loss: 3.1875970363616943\n",
      "Loss: 3.231117010116577\n",
      "Loss: 3.1306827068328857\n",
      "Loss: 3.059819221496582\n",
      "Loss: 3.0998361110687256\n",
      "Loss: 3.1351633071899414\n",
      "Loss: 3.28944993019104\n",
      "Loss: 3.0991814136505127\n",
      "Loss: 3.5516653060913086\n",
      "Loss: 3.2712314128875732\n",
      "Loss: 3.468156099319458\n",
      "Loss: 3.260019063949585\n",
      "Loss: 3.489118814468384\n",
      "Loss: 3.2504477500915527\n",
      "Loss: 3.703897476196289\n",
      "Loss: 3.4489381313323975\n",
      "Loss: 3.0541765689849854\n",
      "Loss: 3.436312437057495\n",
      "Loss: 3.1034367084503174\n",
      "Loss: 3.1453840732574463\n",
      "Loss: 3.4994699954986572\n",
      "Loss: 3.388789415359497\n",
      "Loss: 3.3751027584075928\n",
      "Loss: 3.124847650527954\n",
      "Loss: 3.3787083625793457\n",
      "Loss: 3.3744025230407715\n",
      "Loss: 3.4787790775299072\n",
      "Loss: 3.4800262451171875\n",
      "Loss: 3.2566325664520264\n",
      "Loss: 3.1703734397888184\n",
      "Loss: 3.1468265056610107\n",
      "Loss: 2.895205020904541\n",
      "Loss: 3.1132354736328125\n",
      "Loss: 3.2080941200256348\n",
      "Loss: 3.2819929122924805\n",
      "Loss: 3.3779871463775635\n",
      "Loss: 3.130300521850586\n",
      "Loss: 3.2162084579467773\n",
      "Loss: 3.0635950565338135\n",
      "Loss: 3.048250675201416\n",
      "Loss: 3.2897539138793945\n",
      "Loss: 3.501199245452881\n",
      "Loss: 3.2007927894592285\n",
      "Loss: 3.6096761226654053\n",
      "Loss: 3.1925158500671387\n",
      "Loss: 3.2729544639587402\n",
      "Loss: 3.121428966522217\n",
      "Loss: 3.5087969303131104\n",
      "Loss: 3.2063896656036377\n",
      "Loss: 3.2466166019439697\n",
      "Loss: 3.7001209259033203\n",
      "Loss: 3.3408284187316895\n",
      "Loss: 3.3185412883758545\n",
      "Loss: 3.241096019744873\n",
      "Loss: 3.229497194290161\n",
      "Loss: 3.405341625213623\n",
      "Loss: 3.123955488204956\n",
      "Loss: 3.340547561645508\n",
      "Loss: 3.1857612133026123\n",
      "Loss: 3.22329044342041\n",
      "Loss: 3.231738805770874\n",
      "Loss: 3.2958054542541504\n",
      "Loss: 3.1519765853881836\n",
      "Loss: 3.3477554321289062\n",
      "Loss: 3.2162508964538574\n",
      "Loss: 3.216346025466919\n",
      "Loss: 3.577388286590576\n",
      "Loss: 3.1163291931152344\n",
      "Loss: 3.281968116760254\n",
      "Loss: 3.30330491065979\n",
      "Loss: 3.1197354793548584\n",
      "Loss: 3.219428539276123\n",
      "Loss: 3.396620750427246\n",
      "Loss: 3.318488359451294\n",
      "Loss: 3.1803934574127197\n",
      "Loss: 3.384652614593506\n",
      "Loss: 3.072380781173706\n",
      "Loss: 3.2153680324554443\n",
      "Loss: 3.087815046310425\n",
      "Loss: 3.079939842224121\n",
      "Loss: 3.0683374404907227\n",
      "Loss: 3.2087490558624268\n",
      "Loss: 3.334566831588745\n",
      "Loss: 3.1188559532165527\n",
      "Loss: 3.387401580810547\n",
      "Loss: 2.9337871074676514\n",
      "Loss: 3.0946035385131836\n",
      "Loss: 3.197826385498047\n",
      "Loss: 3.090709924697876\n",
      "Loss: 3.1844048500061035\n",
      "Loss: 3.031407594680786\n",
      "Loss: 3.343001127243042\n",
      "Loss: 3.287735939025879\n",
      "Loss: 3.628749370574951\n",
      "Loss: 3.276125431060791\n",
      "Loss: 3.3326339721679688\n",
      "Loss: 3.0344576835632324\n",
      "Loss: 3.2642016410827637\n",
      "Loss: 3.53399395942688\n",
      "Loss: 3.284188985824585\n",
      "Loss: 3.1328232288360596\n",
      "Loss: 3.1289584636688232\n",
      "Loss: 3.3891849517822266\n",
      "Loss: 3.359579086303711\n",
      "Loss: 3.121370315551758\n",
      "Loss: 3.3540003299713135\n",
      "Loss: 3.1889493465423584\n",
      "Loss: 3.24078369140625\n",
      "Loss: 3.310351848602295\n",
      "Loss: 3.151481866836548\n",
      "Loss: 3.196227788925171\n",
      "Loss: 3.2482030391693115\n",
      "Loss: 3.5665199756622314\n",
      "Loss: 3.2721893787384033\n",
      "Loss: 2.913614273071289\n",
      "Loss: 3.2325849533081055\n",
      "Loss: 3.057049512863159\n",
      "Loss: 3.4139013290405273\n",
      "Loss: 3.4916350841522217\n",
      "Loss: 3.2526416778564453\n",
      "Loss: 3.1987061500549316\n",
      "Loss: 2.9534451961517334\n",
      "Loss: 3.202692747116089\n",
      "Loss: 3.1908645629882812\n",
      "Loss: 3.4338572025299072\n",
      "Loss: 3.0497071743011475\n",
      "Loss: 3.155168294906616\n",
      "Loss: 3.283222198486328\n",
      "Loss: 3.2439794540405273\n",
      "Loss: 3.157639980316162\n",
      "Loss: 3.335310459136963\n",
      "Loss: 3.5018603801727295\n",
      "Loss: 3.126873254776001\n",
      "Loss: 3.4643571376800537\n",
      "Loss: 3.2872462272644043\n",
      "Loss: 3.479050397872925\n",
      "Loss: 3.1434531211853027\n",
      "Loss: 3.2651095390319824\n",
      "Loss: 3.152289628982544\n",
      "Loss: 3.348719358444214\n",
      "Loss: 3.093730926513672\n",
      "Loss: 3.146505355834961\n",
      "Loss: 3.245959997177124\n",
      "Loss: 3.300795793533325\n",
      "Loss: 3.126728057861328\n",
      "Loss: 2.9681596755981445\n",
      "Loss: 3.3310155868530273\n",
      "Loss: 3.1220810413360596\n",
      "Loss: 3.4636547565460205\n",
      "Loss: 3.3293368816375732\n",
      "Loss: 3.136429786682129\n",
      "Loss: 3.2888615131378174\n",
      "Loss: 3.0137786865234375\n",
      "Loss: 3.2396085262298584\n",
      "Loss: 3.1691184043884277\n",
      "Loss: 3.1255972385406494\n",
      "Loss: 3.270503282546997\n",
      "Loss: 3.5420682430267334\n",
      "Loss: 2.973233461380005\n",
      "Loss: 3.1915369033813477\n",
      "Loss: 3.4259653091430664\n",
      "Loss: 3.217661142349243\n",
      "Loss: 3.2327558994293213\n",
      "Loss: 3.248464345932007\n",
      "Loss: 3.4576797485351562\n",
      "Loss: 3.1276371479034424\n",
      "Loss: 3.3928003311157227\n",
      "Loss: 3.4032106399536133\n",
      "Loss: 3.1973583698272705\n",
      "Loss: 3.1121742725372314\n",
      "Loss: 3.041721820831299\n",
      "Loss: 2.994563102722168\n",
      "Loss: 3.065415620803833\n",
      "Loss: 2.9289844036102295\n",
      "Loss: 3.1036646366119385\n",
      "Loss: 3.293586015701294\n",
      "Loss: 3.4659905433654785\n",
      "Loss: 3.4239490032196045\n",
      "Loss: 3.165003538131714\n",
      "Loss: 3.0404982566833496\n",
      "Loss: 3.2513699531555176\n",
      "Loss: 3.118032693862915\n",
      "Loss: 3.4111547470092773\n",
      "Loss: 3.304079532623291\n",
      "Loss: 3.4236061573028564\n",
      "Loss: 3.422396421432495\n",
      "Loss: 3.2641544342041016\n",
      "Loss: 3.448470115661621\n",
      "Loss: 2.9850728511810303\n",
      "Loss: 3.2556731700897217\n",
      "Loss: 3.1342785358428955\n",
      "Loss: 3.260467529296875\n",
      "Loss: 3.1526458263397217\n",
      "Loss: 2.9232237339019775\n",
      "Loss: 3.383979082107544\n",
      "Loss: 3.2449848651885986\n",
      "Loss: 3.2849762439727783\n",
      "Loss: 3.2083749771118164\n",
      "Loss: 3.072765588760376\n",
      "Loss: 3.1581509113311768\n",
      "Loss: 3.1416542530059814\n",
      "Loss: 3.1956872940063477\n",
      "Loss: 3.1900901794433594\n",
      "Loss: 3.0773444175720215\n",
      "Loss: 2.927628755569458\n",
      "Loss: 2.841697931289673\n",
      "Loss: 3.493242025375366\n",
      "Loss: 3.00466251373291\n",
      "Loss: 3.2794532775878906\n",
      "Loss: 3.1873619556427\n",
      "Loss: 3.335874080657959\n",
      "Loss: 3.2782373428344727\n",
      "Loss: 3.063344717025757\n",
      "Loss: 3.527484655380249\n",
      "Loss: 3.1019246578216553\n",
      "Loss: 3.383425712585449\n",
      "Loss: 3.0245368480682373\n",
      "Loss: 3.0235142707824707\n",
      "Loss: 2.8413641452789307\n",
      "Loss: 3.2414562702178955\n",
      "Loss: 3.029574155807495\n",
      "Loss: 3.213430881500244\n",
      "Loss: 3.1495392322540283\n",
      "Loss: 3.1319398880004883\n",
      "Loss: 3.50274658203125\n",
      "Loss: 3.0163424015045166\n",
      "Loss: 3.018065929412842\n",
      "Loss: 3.368415117263794\n",
      "Loss: 3.340143918991089\n",
      "Loss: 3.2469682693481445\n",
      "Loss: 3.1424479484558105\n",
      "Loss: 3.063302755355835\n",
      "Loss: 3.487416982650757\n",
      "Loss: 3.5840182304382324\n",
      "Loss: 3.1940014362335205\n",
      "Loss: 3.1590118408203125\n",
      "Loss: 3.296907901763916\n",
      "Loss: 3.205322265625\n",
      "Loss: 3.2813162803649902\n",
      "Loss: 3.145625114440918\n",
      "Loss: 3.339245319366455\n",
      "Loss: 3.0521509647369385\n",
      "Loss: 3.2158119678497314\n",
      "Loss: 3.030435085296631\n",
      "Loss: 3.4841558933258057\n",
      "Loss: 3.371453046798706\n",
      "Loss: 3.4698569774627686\n",
      "Loss: 3.0637052059173584\n",
      "Loss: 3.025076389312744\n",
      "Loss: 3.0148990154266357\n",
      "Loss: 3.192732572555542\n",
      "Loss: 2.8998613357543945\n",
      "Loss: 3.3399524688720703\n",
      "Loss: 3.2523467540740967\n",
      "Loss: 3.416466236114502\n",
      "Loss: 3.2239646911621094\n",
      "Loss: 3.1347975730895996\n",
      "Loss: 3.421267509460449\n",
      "Loss: 3.3894262313842773\n",
      "Loss: 2.9847216606140137\n",
      "Loss: 3.3140368461608887\n",
      "Loss: 3.3844590187072754\n",
      "Loss: 3.1859614849090576\n",
      "Loss: 3.792107582092285\n",
      "Loss: 3.135554313659668\n",
      "Loss: 3.1627180576324463\n",
      "Loss: 2.8390820026397705\n",
      "Loss: 3.2182304859161377\n",
      "Loss: 3.3384592533111572\n",
      "Loss: 3.2683053016662598\n",
      "Loss: 3.1918017864227295\n",
      "Loss: 2.9649083614349365\n",
      "Loss: 2.927375316619873\n",
      "Loss: 2.9997823238372803\n",
      "Loss: 3.191467046737671\n",
      "Loss: 3.1447834968566895\n",
      "Loss: 3.0368595123291016\n",
      "Loss: 3.179656982421875\n",
      "Loss: 3.286573886871338\n",
      "Loss: 3.0389292240142822\n",
      "Loss: 3.103724956512451\n",
      "Loss: 3.337405204772949\n",
      "Loss: 3.45013165473938\n",
      "Loss: 3.109269142150879\n",
      "Loss: 3.0129222869873047\n",
      "Loss: 3.2714428901672363\n",
      "Loss: 3.110255241394043\n",
      "Loss: 3.057543992996216\n",
      "Loss: 3.5018129348754883\n",
      "Loss: 3.2785913944244385\n",
      "Loss: 3.2606348991394043\n",
      "Loss: 3.1593503952026367\n",
      "Loss: 3.256887674331665\n",
      "Loss: 3.038623809814453\n",
      "Loss: 3.271916627883911\n",
      "Loss: 3.0452051162719727\n",
      "Loss: 3.318847417831421\n",
      "Loss: 3.1708316802978516\n",
      "Loss: 3.4080917835235596\n",
      "Loss: 3.1621718406677246\n",
      "Loss: 3.111635208129883\n",
      "Loss: 3.03836727142334\n",
      "Loss: 3.1842117309570312\n",
      "Loss: 3.379927635192871\n",
      "Loss: 3.026622772216797\n",
      "Loss: 3.063101053237915\n",
      "Loss: 3.153855562210083\n",
      "Loss: 3.2105531692504883\n",
      "Loss: 2.8833627700805664\n",
      "Loss: 3.1148500442504883\n",
      "Loss: 3.5352165699005127\n",
      "Loss: 3.4825146198272705\n",
      "Loss: 3.179103374481201\n",
      "Loss: 3.0780107975006104\n",
      "Loss: 3.075470447540283\n",
      "Loss: 2.9052865505218506\n",
      "Loss: 3.266752004623413\n",
      "Loss: 3.3525712490081787\n",
      "Loss: 3.0868148803710938\n",
      "Loss: 3.2387747764587402\n",
      "Loss: 3.132697105407715\n",
      "Loss: 3.035069704055786\n",
      "Loss: 3.3788483142852783\n",
      "Loss: 3.2274293899536133\n",
      "Loss: 3.0334959030151367\n",
      "Loss: 3.139448881149292\n",
      "Loss: 3.0634777545928955\n",
      "Loss: 3.191291093826294\n",
      "Loss: 3.217806339263916\n",
      "Loss: 3.199873685836792\n",
      "Loss: 3.3913536071777344\n",
      "Loss: 3.033611536026001\n",
      "Loss: 3.0783002376556396\n",
      "Loss: 3.0976932048797607\n",
      "Loss: 2.974299430847168\n",
      "Loss: 3.3832342624664307\n",
      "Loss: 3.0172297954559326\n",
      "Loss: 3.226828098297119\n",
      "Loss: 3.4037086963653564\n",
      "Loss: 3.299448013305664\n",
      "Loss: 3.2498292922973633\n",
      "Loss: 3.4705448150634766\n",
      "Loss: 3.2347114086151123\n",
      "Loss: 2.9874074459075928\n",
      "Loss: 2.9644110202789307\n",
      "Loss: 3.1560163497924805\n",
      "Loss: 3.0434675216674805\n",
      "Loss: 3.0711519718170166\n",
      "Loss: 3.2207677364349365\n",
      "Loss: 2.8468639850616455\n",
      "Loss: 3.084716320037842\n",
      "Loss: 3.1716208457946777\n",
      "Loss: 3.0456318855285645\n",
      "Loss: 2.9136545658111572\n",
      "Loss: 3.0848090648651123\n",
      "Loss: 3.1191256046295166\n",
      "Loss: 3.3068480491638184\n",
      "Loss: 3.252192974090576\n",
      "Loss: 2.981484889984131\n",
      "Loss: 3.2226297855377197\n",
      "Loss: 2.7876648902893066\n",
      "Loss: 2.9048099517822266\n",
      "Loss: 3.0029571056365967\n",
      "Loss: 2.952802896499634\n",
      "Loss: 3.215735912322998\n",
      "Loss: 3.08979868888855\n",
      "Loss: 3.0184710025787354\n",
      "Loss: 3.0734853744506836\n",
      "Loss: 3.491442918777466\n",
      "Loss: 3.1019811630249023\n",
      "Loss: 3.235274314880371\n",
      "Loss: 3.4821386337280273\n",
      "Loss: 3.248204469680786\n",
      "Loss: 3.269843339920044\n",
      "Loss: 3.088299036026001\n",
      "Loss: 3.012874126434326\n",
      "Loss: 2.9219653606414795\n",
      "Loss: 3.377319812774658\n",
      "Loss: 3.3003721237182617\n",
      "Loss: 3.2421045303344727\n",
      "Loss: 3.064288377761841\n",
      "Loss: 3.075350761413574\n",
      "Loss: 2.899688482284546\n",
      "Loss: 3.0641543865203857\n",
      "Loss: 3.067667007446289\n",
      "Loss: 3.1041834354400635\n",
      "Loss: 3.2082104682922363\n",
      "Loss: 3.0670361518859863\n",
      "Loss: 3.0038254261016846\n",
      "Loss: 3.1801869869232178\n",
      "Loss: 3.0775012969970703\n",
      "Loss: 2.979255437850952\n",
      "Loss: 3.2270281314849854\n",
      "Loss: 2.8525896072387695\n",
      "Loss: 2.923957586288452\n",
      "Loss: 2.8540687561035156\n",
      "Loss: 3.0857670307159424\n",
      "Loss: 3.21622633934021\n",
      "Loss: 3.5719430446624756\n",
      "Loss: 3.235682487487793\n",
      "Loss: 3.2055230140686035\n",
      "Loss: 3.1938657760620117\n",
      "Loss: 2.9939560890197754\n",
      "Loss: 3.330599308013916\n",
      "Loss: 2.8936986923217773\n",
      "Loss: 2.886767625808716\n",
      "Loss: 3.1674489974975586\n",
      "Loss: 3.1083056926727295\n",
      "Loss: 3.011693000793457\n",
      "Loss: 3.3349430561065674\n",
      "Loss: 2.988678455352783\n",
      "Loss: 2.8956990242004395\n",
      "Loss: 2.873298168182373\n",
      "Loss: 3.1865694522857666\n",
      "Loss: 3.234313726425171\n",
      "Loss: 3.0749571323394775\n",
      "Loss: 3.077455997467041\n",
      "Loss: 3.1183230876922607\n",
      "Loss: 2.936920166015625\n",
      "Loss: 3.286219358444214\n",
      "Loss: 3.1068365573883057\n",
      "Loss: 3.0432615280151367\n",
      "Loss: 3.2340142726898193\n",
      "Loss: 2.89060378074646\n",
      "Loss: 3.1666877269744873\n",
      "Loss: 3.018127202987671\n",
      "Loss: 3.23421049118042\n",
      "Loss: 3.0995819568634033\n",
      "Loss: 2.9598922729492188\n",
      "Loss: 3.222330093383789\n",
      "Loss: 3.072169542312622\n",
      "Loss: 2.9800007343292236\n",
      "Loss: 3.2353591918945312\n",
      "Loss: 3.234461784362793\n",
      "Loss: 3.1999731063842773\n",
      "Loss: 3.056812047958374\n",
      "Loss: 3.0303163528442383\n",
      "Loss: 2.9027369022369385\n",
      "Loss: 3.601299285888672\n",
      "Loss: 3.1164450645446777\n",
      "Loss: 3.0165481567382812\n",
      "Loss: 2.91471791267395\n",
      "Loss: 2.9254150390625\n",
      "Loss: 3.065223217010498\n",
      "Loss: 2.99072265625\n",
      "Loss: 3.1061930656433105\n",
      "val_loss at 1 epoch: 4.0633411066872736\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "validate(0)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        print(f'Loss: {loss.item()}')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    validate(epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LUIMalJLDM3S",
    "outputId": "6fac1c0d-a8ee-4669-8098-058b5124c5fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1061930656433105\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UczycQOd-NTf"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('./sft_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMh8YdVBAUeW",
    "outputId": "407e23b6-ed08-47bf-fb7a-0cc66caaef09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.from_pretrained('./sft_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0zT_kHHAkau"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
